"""
æ§‹é€ åŒ–ãƒ­ã‚°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆJSON Lineså½¢å¼ï¼‰
"""
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
import streamlit as st

# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
LOG_DIR = Path("logs")
LOG_DIR.mkdir(exist_ok=True)

class JSONLinesLogger:
    """JSON Lineså½¢å¼ã§ãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, log_name: str = "app"):
        self.log_name = log_name
        self.log_file = self._get_log_file()
        
        # æ¨™æº–ãƒ­ã‚¬ãƒ¼è¨­å®š
        self.logger = logging.getLogger(f"jsonl_{log_name}")
        self.logger.setLevel(logging.INFO)
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒæ—¢ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
    
    def _get_log_file(self) -> Path:
        """æ—¥ä»˜ãƒ™ãƒ¼ã‚¹ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        today = datetime.now().strftime("%Y%m%d")
        return LOG_DIR / f"{self.log_name}-{today}.jsonl"
    
    def log_event(self, event_type: str, data: Dict[str, Any], level: str = "info", user_session: Optional[str] = None) -> None:
        """
        ã‚¤ãƒ™ãƒ³ãƒˆã‚’JSON Lineså½¢å¼ã§ãƒ­ã‚°å‡ºåŠ›
        
        Args:
            event_type: ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆä¾‹: "search", "draft_save", "error"ï¼‰
            data: ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿
            level: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«
            user_session: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
        """
        try:
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—ï¼ˆStreamlitç’°å¢ƒï¼‰
            if user_session is None and hasattr(st, 'session_state'):
                try:
                    if 'session_id' not in st.session_state:
                        import uuid
                        st.session_state.session_id = str(uuid.uuid4())[:8]
                    user_session = st.session_state.session_id
                except:
                    user_session = "unknown"
            
            # ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªãƒ¼ä½œæˆ
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "event_type": event_type,
                "level": level.upper(),
                "session_id": user_session,
                "data": data
            }
            
            # JSON Lineså½¢å¼ã§ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
            with open(self.log_file, 'a', encoding='utf-8') as f:
                json.dump(log_entry, f, ensure_ascii=False, separators=(',', ':'))
                f.write('\n')
            
            # æ¨™æº–ãƒ­ã‚°ã«ã‚‚å‡ºåŠ›
            log_message = f"[{event_type}] {json.dumps(data, ensure_ascii=False, separators=(',', ':'))}"
            
            if level.lower() == "error":
                self.logger.error(log_message)
            elif level.lower() == "warning":
                self.logger.warning(log_message)
            elif level.lower() == "debug":
                self.logger.debug(log_message)
            else:
                self.logger.info(log_message)
                
        except Exception as e:
            # ãƒ­ã‚°å‡ºåŠ›ã‚¨ãƒ©ãƒ¼ã¯æ¨™æº–ãƒ­ã‚°ã«ã®ã¿è¨˜éŒ²
            self.logger.error(f"Error writing JSON log: {e}")
    
    def log_search(self, keyword: str, results_count: int, processing_time: float) -> None:
        """æ¤œç´¢ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒ­ã‚°"""
        self.log_event("search", {
            "keyword": keyword,
            "results_count": results_count,
            "processing_time_seconds": processing_time,
            "timestamp": datetime.now().isoformat()
        })
    
    def log_draft_save(self, item_id: str, title: str, profit_jpy: float, success: bool) -> None:
        """ä¸‹æ›¸ãä¿å­˜ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒ­ã‚°"""
        self.log_event("draft_save", {
            "item_id": item_id,
            "title": title,
            "profit_jpy": profit_jpy,
            "success": success,
            "timestamp": datetime.now().isoformat()
        })
    
    def log_error(self, error_type: str, error_message: str, context: Dict[str, Any] = None) -> None:
        """ã‚¨ãƒ©ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒ­ã‚°"""
        error_data = {
            "error_type": error_type,
            "error_message": error_message,
            "timestamp": datetime.now().isoformat()
        }
        
        if context:
            error_data["context"] = context
        
        self.log_event("error", error_data, level="error")
    
    def log_user_action(self, action: str, details: Dict[str, Any] = None) -> None:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒ­ã‚°"""
        action_data = {
            "action": action,
            "timestamp": datetime.now().isoformat()
        }
        
        if details:
            action_data["details"] = details
        
        self.log_event("user_action", action_data)
    
    def log_api_call(self, api_name: str, endpoint: str, success: bool, response_time: float, status_code: int = None) -> None:
        """APIå‘¼ã³å‡ºã—ã‚’ãƒ­ã‚°"""
        self.log_event("api_call", {
            "api_name": api_name,
            "endpoint": endpoint,
            "success": success,
            "response_time_seconds": response_time,
            "status_code": status_code,
            "timestamp": datetime.now().isoformat()
        })


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ­ã‚¬ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_app_logger = None

def get_app_logger() -> JSONLinesLogger:
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—"""
    global _app_logger
    if _app_logger is None:
        _app_logger = JSONLinesLogger("app")
    return _app_logger


def read_log_entries(log_date: str = None, event_type: str = None, limit: int = 100) -> list:
    """
    ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’èª­ã¿è¾¼ã¿
    
    Args:
        log_date: èª­ã¿è¾¼ã‚€æ—¥ä»˜ï¼ˆYYYYMMDDå½¢å¼ï¼‰
        event_type: ãƒ•ã‚£ãƒ«ã‚¿ã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—
        limit: æœ€å¤§èª­ã¿è¾¼ã¿ä»¶æ•°
        
    Returns:
        list: ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã®ãƒªã‚¹ãƒˆ
    """
    try:
        if log_date is None:
            log_date = datetime.now().strftime("%Y%m%d")
        
        log_file = LOG_DIR / f"app-{log_date}.jsonl"
        
        if not log_file.exists():
            return []
        
        entries = []
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        entry = json.loads(line)
                        
                        # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿
                        if event_type and entry.get("event_type") != event_type:
                            continue
                        
                        entries.append(entry)
                        
                        # åˆ¶é™ãƒã‚§ãƒƒã‚¯
                        if len(entries) >= limit:
                            break
                            
                    except json.JSONDecodeError:
                        continue
        
        # æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
        entries.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
        
        return entries
        
    except Exception as e:
        logging.error(f"Error reading log entries: {e}")
        return []


def get_log_stats(log_date: str = None) -> Dict[str, Any]:
    """
    ãƒ­ã‚°çµ±è¨ˆã‚’å–å¾—
    
    Args:
        log_date: çµ±è¨ˆå¯¾è±¡æ—¥ä»˜ï¼ˆYYYYMMDDå½¢å¼ï¼‰
        
    Returns:
        Dict: çµ±è¨ˆæƒ…å ±
    """
    try:
        entries = read_log_entries(log_date, limit=10000)
        
        if not entries:
            return {"total_events": 0}
        
        # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
        event_counts = {}
        error_count = 0
        
        for entry in entries:
            event_type = entry.get("event_type", "unknown")
            event_counts[event_type] = event_counts.get(event_type, 0) + 1
            
            if entry.get("level") == "ERROR":
                error_count += 1
        
        # æ™‚é–“å¸¯åˆ¥é›†è¨ˆ
        hourly_counts = {}
        for entry in entries:
            try:
                timestamp = datetime.fromisoformat(entry.get("timestamp", ""))
                hour = timestamp.hour
                hourly_counts[hour] = hourly_counts.get(hour, 0) + 1
            except:
                continue
        
        stats = {
            "total_events": len(entries),
            "event_types": event_counts,
            "error_count": error_count,
            "hourly_distribution": hourly_counts,
            "date": log_date or datetime.now().strftime("%Y%m%d")
        }
        
        return stats
        
    except Exception as e:
        logging.error(f"Error getting log stats: {e}")
        return {"error": str(e)}


def cleanup_old_logs(days_to_keep: int = 30) -> int:
    """
    å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    
    Args:
        days_to_keep: ä¿æŒæ—¥æ•°
        
    Returns:
        int: å‰Šé™¤ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°
    """
    try:
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        deleted_count = 0
        
        for log_file in LOG_DIR.glob("*.jsonl"):
            if log_file.stat().st_mtime < cutoff_date.timestamp():
                log_file.unlink()
                deleted_count += 1
        
        logging.info(f"Cleaned up {deleted_count} old log files")
        return deleted_count
        
    except Exception as e:
        logging.error(f"Error cleaning up old logs: {e}")
        return 0


# Streamlitç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
def display_log_summary() -> None:
    """Streamlitã§ãƒ­ã‚°è¦ç´„ã‚’è¡¨ç¤º"""
    try:
        today_stats = get_log_stats()
        
        if today_stats.get("total_events", 0) > 0:
            st.markdown("### ğŸ“Š ä»Šæ—¥ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("ç·ã‚¤ãƒ™ãƒ³ãƒˆæ•°", today_stats["total_events"])
            
            with col2:
                st.metric("ã‚¨ãƒ©ãƒ¼æ•°", today_stats.get("error_count", 0))
            
            with col3:
                search_count = today_stats.get("event_types", {}).get("search", 0)
                st.metric("æ¤œç´¢å›æ•°", search_count)
            
            # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ¥è©³ç´°
            if today_stats.get("event_types"):
                st.markdown("**ã‚¤ãƒ™ãƒ³ãƒˆç¨®åˆ¥:**")
                for event_type, count in today_stats["event_types"].items():
                    st.write(f"- {event_type}: {count}å›")
        else:
            st.info("ä»Šæ—¥ã®ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“")
            
    except Exception as e:
        st.error(f"ãƒ­ã‚°è¦ç´„è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")


# ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
def test_logging_utils():
    """ãƒ­ã‚°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã®ãƒ†ã‚¹ãƒˆ"""
    print("=== Logging Utils Test ===")
    
    logger = get_app_logger()
    
    # å„ç¨®ãƒ­ã‚°ãƒ†ã‚¹ãƒˆ
    logger.log_search("Nintendo Switch", 15, 0.5)
    logger.log_draft_save("test123", "Test Item", 2500.0, True)
    logger.log_user_action("button_click", {"button": "search"})
    logger.log_api_call("eBay", "/search", True, 1.2, 200)
    logger.log_error("validation_error", "Invalid item data")
    
    print("Test logs written")
    
    # ãƒ­ã‚°èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
    entries = read_log_entries(limit=5)
    print(f"Read {len(entries)} log entries")
    
    # çµ±è¨ˆãƒ†ã‚¹ãƒˆ
    stats = get_log_stats()
    print(f"Log stats: {stats}")


if __name__ == "__main__":
    from datetime import timedelta
    test_logging_utils()
